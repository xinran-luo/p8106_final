---
title: "P8106 Data Science II Final Project"
author: "Stephanie Zhen, Xinran Luo, Ran An"
date: "5/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r message=FALSE}
library(tidyverse)
library(readr)
library(Amelia)
library(DataExplorer)
library(caret)
library(pROC)
library(ROCR)
library(klaR)
library(AppliedPredictiveModeling)
library(naivebayes)
library(rpart.plot)
library(e1071)
```

age: age in years

sex: (1 = male; 0 = female)

cp: chest pain type (typical angina, atypical angina, non-angina, or asymptomatic angina)

trestbps: resting blood pressure (in mm Hg on admission to the hospital)

chol: serum cholestoral in mg/dl

fbs: Fasting blood sugar (< 120 mg/dl or > 120 mg/dl) (1 = true; 0 = false)

restecg: resting electrocardiographic results (normal, ST-T wave abnormality, or left ventricular hypertrophy)

thalach: Max. heart rate achieved during thalium stress test

exang: Exercise induced angina (1 = yes; 0 = no)

oldpeak: ST depression induced by exercise relative to rest

slope: Slope of peak exercise ST segment (0 = upsloping, 1 = flat, or 2 = downsloping)

ca: number of major vessels (0-3) colored by flourosopy 4 = NA

thal: Thalium stress test result 3 = normal; 6 = fixed defect; 7 = reversable defect 0 = NA

target: Heart disease status 1 or 0 (0 = heart disease 1 = asymptomatic)

### Load the data

```{r message=FALSE, warning=FALSE}
heart = read_csv("./data/heart.csv")
head(heart)

summary(heart)

missmap(heart)

```


###Data Cleaning:
```{r}
heart_nomiss = heart %>%
  filter(thal != 0 & ca != 4) %>% 
  mutate(
    sex = case_when(
     sex == 1 ~ "male",
    sex == 0 ~ "female"
           ),
    fbs = case_when(
      fbs == 1 ~ "true",
      fbs == 0 ~ "false"
            ),
    exang = case_when(
      exang == 1 ~ "yes",
      exang == 0 ~ "no"
            ),
    cp = case_when(
      cp == 3 ~ "typical_angina",
      cp == 1 ~ "atypical_angina",
      cp == 2 ~ "non_anginal",
      cp == 0 ~ "asymptomatic_angina"
          ),
    restecg = case_when(
      restecg == 2 ~ "wave_abnormality",
      restecg == 1 ~ "normal",
      restecg == 0 ~ "hypertrophy"
              ),
    target = case_when(
      target == 1 ~ "asymptomatic",
      target == 0 ~ "heart_disease"
              ),
    slope = case_when(
      slope == 2 ~ "upsloping",
      slope == 1 ~ "flat",
      slope == 0 ~ "downsloping"
    ),
    thal = case_when(
      thal == 3 ~ "reversable_defect",
      thal == 2 ~ "normal",
      thal == 1 ~ "fixed_defect"
    ),
    sex = as.factor(sex),
    fbs = as.factor(fbs),
    exang = as.factor(exang),
    cp = as.factor(cp),
    slope = as.factor(slope),
    ca = as.factor(ca),
    thal = as.factor(thal),
    restecg = as.factor(restecg),
    target = as.factor(target)
  )
```


```{r}
plot_density(heart_nomiss)
```

```{r}
plot_bar(heart_nomiss)
```


```{r}
### Age vs outcome(target)
ggplot(data = heart_nomiss, aes(x = target, y = age)) + 
  geom_boxplot() +
  labs(x = "Heart Disease Status",
       y = "Age",
       title = "Heart Disease by Age")

### CP vs outcome(target)
#ggplot(data = heart_nomiss, aes(x = cp,
                                #fill = target)) + 
  #geom_bar(position = position_dodge())

### Gender vs outcome(target)
ggplot(data = heart_nomiss, aes(x = sex,
                                fill = target)) + 
  geom_bar(position = position_dodge()) + 
  labs(x = "Gender",
       y = "Count",
       title = "Heart Disease by Gender")
```


###Partition the data for training
```{r}
set.seed(1)
train.indices = createDataPartition(y = heart_nomiss$target, p = 0.7, list = FALSE)

training = heart_nomiss[train.indices,]
testing = heart_nomiss[-train.indices,]


control = trainControl(method = "cv",
                        summaryFunction = twoClassSummary,
                        classProbs = TRUE)
```


###Naive Bayes (NB)
```{r warning=FALSE}
set.seed(1)

nbGrid = expand.grid(usekernel = c(FALSE,TRUE),fL = 1,adjust = seq(.2, 1, by = .2))
nb.fit = train(x = heart_nomiss[train.indices,1:13],y = heart_nomiss$target[train.indices],method = "nb",tuneGrid = nbGrid,metric = "ROC",trControl = control)
plot(nb.fit)

```


## rpart
```{r}

set.seed(1)
rpart.fit = train(target ~ .,
                  data = training,
                  method = "rpart",
                  tuneGrid = data.frame(cp = exp(seq(-9, -5, len = 30))),
                  trControl = control,
                  metric = "ROC")

ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)

##Predict using train data
rpart_train = predict(rpart.fit,
                      newdata = training,
                      type = "prob")[,2]

roc_rpart_train = roc(training$target, rpart_train)
plot(roc_rpart_train, legacy.axes = TRUE, print.auc = TRUE)
plot(roc_rpart_train, add = TRUE, col = 3)


##Predict using test data
rpart_test = predict(rpart.fit,
                      newdata = testing,
                      type = "prob")[,2]

roc_rpart_test = roc(testing$target, rpart_test)
plot(roc_rpart_test, legacy.axes = TRUE, print.auc = TRUE)
plot(roc_rpart_test, add = TRUE, col = 3)

```

### Random Forest
```{r}
rf_grid = expand.grid(mtry = 3:7,
                      splitrule = "gini",
                      min.node.size = 3:7)
set.seed(1)
rf.fit = train(target ~., 
               data = training,
               method = "ranger",
               tuneGrid = rf_grid,
               metric = "ROC",
               importance = "permutation",
               trControl = control)

ggplot(rf.fit, highlight = TRUE)

##Predicting using training data
rf_train = predict(rf.fit,
                  newdata = training, 
                  type = "prob")[,2]

roc_rf_train = roc(training$target, rf_train)
plot(roc_rf_train, legacy.axes = TRUE, print.auc = TRUE)
plot(roc_rf_train, add = TRUE, col = 3)

```


### Variable importance using random forest
```{r}
rf_imp = varImp(rf.fit)
plot(rf_imp)
```



### support vector classifier (linear kernel)
```{r warning=FALSE, message=FALSE}
set.seed(1)
# fit model
svml.fit = train(target ~ ., 
                  data = training, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-5,-1,len=50))),
                  trControl = control)
# model output
svml.final = svml.fit$finalModel
# best tunning parameter
svml.fit$bestTune
# Accuracy plot
ggplot(svml.fit, highlight = TRUE)

```

### Logistic regression
```{r warning=FALSE, message=FALSE}
set.seed(1)
glm.fit = train(target ~., data=training, method='glm', trControl = control)

```

###Model Selection

```{r}
set.seed(1)
resamp = resamples(list(  nb =nb.fit,
                          rpart = rpart.fit,
                          rf = rf.fit,
                          svml = svml.fit,
                          glm = glm.fit))
summary(resamp)
bwplot(resamp)
```

Support vector classifier (linear kernel) performs best.

### Performance evaluation

```{r warning=FALSE, message=FALSE}

svml.pred <- predict(svml.fit, newdata = testing, type = "prob")[,1]
roc.svml = roc(testing$target, svml.pred)
plot(roc.svml,print.auc = TRUE)



pred.linear <- predict(svml.fit, newdata = testing)
confusionMatrix(data = pred.linear,reference = testing$target)

```




